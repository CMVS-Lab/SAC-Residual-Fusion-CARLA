{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import carla \n",
    "# client = carla.Client(\"localhost\", 2000)\n",
    "# world = client.load_world('Town01')\n",
    "import gymnasium as gym\n",
    "from carla_env import CarEnv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class CARLA_G(gym.Env):\n",
    "    def __init__(self, ):\n",
    "        super(CARLA_G, self).__init__()\n",
    "        self.env = CarEnv()\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape = (2, ), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low = -np.inf, high = np.inf, shape=(16, ), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        [new_image, new_state], reward, done, info = self.env.step(action)\n",
    "        return new_state.astype(np.float32), reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed = None, options = {}):\n",
    "        image, state = self.env.reset()\n",
    "        return state.astype(np.float32), {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "env = CARLA_G()\n",
    "# check_env(env, warn=True)\n",
    "# model = DDPG(\"MlpPolicy\", env)\n",
    "# model.learn(total_timesteps=10000, log_interval=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, PPO, DDPG\n",
    "model = SAC.load(\"SAC_model_run1_74\", print_system_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SAC(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, PPO, DDPG\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "model = SAC.load(\"SAC_model_run1_74\", print_system_info=True)\n",
    "model.env = model2.env\n",
    "\n",
    "for i in range(1000):\n",
    "    name = \"SAC_model_run2_\"+str(i+1)\n",
    "    print(name)\n",
    "    model.learn(total_timesteps=10000, log_interval=4)\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "# Evaluate the agent\n",
    "episode_reward = 0\n",
    "for _ in range(10000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if terminated or truncated or info.get(\"is_success\", False):\n",
    "        print(\"Reward:\", episode_reward, \"Success?\", info.get(\"is_success\", False))\n",
    "        episode_reward = 0.0\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARLA_G_fusion(gym.Env):\n",
    "    def __init__(self, ):\n",
    "        super(CARLA_G_fusion, self).__init__()\n",
    "        self.env = CarEnv()\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape = (2, ), dtype=np.float32)\n",
    "        # self.observation_space = gym.spaces.Box(low = -np.inf, high = np.inf, shape=(16, ), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Dict({\"image\": gym.spaces.Box(low=0, high=1,shape=(3,60,160), dtype=np.float32), \"tracking\": gym.spaces.Box(low = -np.inf, high = np.inf, shape=(16, ), dtype=np.float32)})\n",
    "\n",
    "    def step(self, action):\n",
    "        [new_image, new_state], reward, done, info = self.env.step(action)\n",
    "        first_ch_new_image = np.moveaxis(new_image,-1,0)/255\n",
    "        return {\"image\":first_ch_new_image.astype(np.float32), \"tracking\":new_state.astype(np.float32)}, reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed = None, options = {}):\n",
    "        image, state = self.env.reset()\n",
    "        first_ch_image = np.moveaxis(image,-1,0)/255\n",
    "\n",
    "        return {\"image\":first_ch_image.astype(np.float32),\"tracking\":state.astype(np.float32)}, {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "env = CARLA_G_fusion()\n",
    "town_name = 'Town01'\n",
    "env.env.town_name = town_name\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, feature_dim=16):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(feature_dim, feature_dim, 3, padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(feature_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(feature_dim, feature_dim, 3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(feature_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(feature_dim, feature_dim, 3, padding='same')\n",
    "        self.bn3 = nn.BatchNorm2d(feature_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # out = self.conv2(out)\n",
    "        # out = self.bn2(out)\n",
    "        # out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ResidualBlock()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(3, 16, 3, padding='same')(torch.rand(1,3,60,160)).shape, net(torch.rand(1,16,60,160)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(3, 16, 3, padding='same')(torch.rand(1,3,60,160)).shape, resnet18(torch.rand(1,3,60,160)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet_18(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels, num_classes):\n",
    "        \n",
    "        super(ResNet_18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "        \n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            Block(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            Block(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "model = ResNet_18(3, 100)\n",
    "nn.Conv2d(3, 16, 3, padding='same')(torch.rand(1,3,60,160)).shape, model(torch.rand(1,3,60,160)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torchvision.models import resnet18, ResNet18_Weights\n",
    "# model = resnet18().train()\n",
    "# model.fc=nn.Linear(512,100)\n",
    "\n",
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, feature_dim=16):\n",
    "        # We do not know features-dim here before going over all the items,\n",
    "        # so put something dummy for now. PyTorch requires calling\n",
    "        # nn.Module.__init__ before adding modules\n",
    "        super().__init__(observation_space, features_dim=1)\n",
    "\n",
    "        extractors = {}\n",
    "\n",
    "        total_concat_size = 0\n",
    "        # We need to know size of the output of this extractor,\n",
    "        # so go over all the spaces and compute output feature sizes\n",
    "        for key, subspace in observation_space.spaces.items():\n",
    "            if key == \"image\":\n",
    "                # We will just downsample one channel of the image by 4x4 and flatten.\n",
    "                # Assume the image is single-channel (subspace.shape[0] == 0)\n",
    "                # extractors[key] = nn.Sequential(nn.Conv2d(3, feature_dim, 3, padding='same'), ResidualBlock(), nn.Flatten(), nn.Linear(feature_dim*60*160, 600))\n",
    "                extractors[key] = nn.Sequential(ResNet_18(3, 100)).to('cuda:0')\n",
    "                total_concat_size += subspace.shape[1] // 4 * subspace.shape[2] // 4\n",
    "            elif key == \"tracking\":\n",
    "                # Run through a simple MLP\n",
    "                extractors[key] = nn.Linear(subspace.shape[0], 16).to('cuda:0')\n",
    "                total_concat_size += 16\n",
    "\n",
    "        self.extractors = nn.ModuleDict(extractors).to('cuda:0')\n",
    "\n",
    "        # Update the features dim manually\n",
    "        self._features_dim = total_concat_size\n",
    "\n",
    "    def forward(self, observations) -> torch.Tensor:\n",
    "        encoded_tensor_list = []\n",
    "\n",
    "        # self.extractors contain nn.Modules that do all the processing.\n",
    "        for key, extractor in self.extractors.items():\n",
    "            encoded_tensor_list.append(extractor(observations[key]))\n",
    "        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "        # print(torch.cat(encoded_tensor_list, dim=1).shape)\n",
    "        return torch.cat(encoded_tensor_list, dim=1).to('cuda:0')\n",
    "    \n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCombinedExtractor,\n",
    "    # features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "model = SAC(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, tensorboard_log=\"./tmp/sac_gpu/\", verbose=1, buffer_size=10000)\n",
    "# SAC\n",
    "model.policy.actor.latent_pi[0] = torch.nn.Linear(116,256)\n",
    "model.policy.critic.qf0[0] = torch.nn.Linear(118,256)\n",
    "model.policy.critic.qf1[0] = torch.nn.Linear(118,256)\n",
    "model.policy.critic_target.qf0[0] = torch.nn.Linear(118,256)\n",
    "model.policy.critic_target.qf1[0] = torch.nn.Linear(118,256)\n",
    "model.policy = model.policy.to('cuda:0')\n",
    "# PPO\n",
    "# model.policy.mlp_extractor.policy_net[0] = torch.nn.Linear(1816,64)\n",
    "# model.policy.mlp_extractor.value_net[0] = torch.nn.Linear(1816,64)\n",
    "\n",
    "model.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gymnasium import spaces\n",
    "\n",
    "# from stable_baselines3 import PPO, SAC\n",
    "# from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "# class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "#     def __init__(self, observation_space: gym.spaces.Dict):\n",
    "#         # We do not know features-dim here before going over all the items,\n",
    "#         # so put something dummy for now. PyTorch requires calling\n",
    "#         # nn.Module.__init__ before adding modules\n",
    "#         super().__init__(observation_space, features_dim=1)\n",
    "\n",
    "#         extractors = {}\n",
    "\n",
    "#         total_concat_size = 0\n",
    "#         # We need to know size of the output of this extractor,\n",
    "#         # so go over all the spaces and compute output feature sizes\n",
    "#         for key, subspace in observation_space.spaces.items():\n",
    "#             if key == \"image\":\n",
    "#                 # We will just downsample one channel of the image by 4x4 and flatten.\n",
    "#                 # Assume the image is single-channel (subspace.shape[0] == 0)\n",
    "#                 extractors[key] = nn.Sequential(nn.MaxPool2d(4), nn.Flatten())\n",
    "#                 total_concat_size += subspace.shape[1] // 4 * subspace.shape[2] // 4\n",
    "#             elif key == \"tracking\":\n",
    "#                 # Run through a simple MLP\n",
    "#                 extractors[key] = nn.Linear(subspace.shape[0], 16)\n",
    "#                 total_concat_size += 16\n",
    "\n",
    "#         self.extractors = nn.ModuleDict(extractors)\n",
    "\n",
    "#         # Update the features dim manually\n",
    "#         self._features_dim = total_concat_size\n",
    "\n",
    "#     def forward(self, observations) -> torch.Tensor:\n",
    "#         encoded_tensor_list = []\n",
    "\n",
    "#         # self.extractors contain nn.Modules that do all the processing.\n",
    "#         for key, extractor in self.extractors.items():\n",
    "#             encoded_tensor_list.append(extractor(observations[key]))\n",
    "#         # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n",
    "#         # print(torch.cat(encoded_tensor_list, dim=1).shape)\n",
    "#         return torch.cat(encoded_tensor_list, dim=1)\n",
    "    \n",
    "# policy_kwargs = dict(\n",
    "#     features_extractor_class=CustomCombinedExtractor,\n",
    "#     # features_extractor_kwargs=dict(features_dim=128),\n",
    "# )\n",
    "# model = SAC(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, tensorboard_log=\"./tmp/sac/\", verbose=1, buffer_size=10000)\n",
    "\n",
    "# # SAC\n",
    "# model.policy.actor.latent_pi[0] = torch.nn.Linear(1816,256)\n",
    "# model.policy.critic.qf0[0] = torch.nn.Linear(1818,256)\n",
    "# model.policy.critic.qf1[0] = torch.nn.Linear(1818,256)\n",
    "# model.policy.critic_target.qf0[0] = torch.nn.Linear(1818,256)\n",
    "# model.policy.critic_target.qf1[0] = torch.nn.Linear(1818,256)\n",
    "\n",
    "# # PPO\n",
    "# # model.policy.mlp_extractor.policy_net[0] = torch.nn.Linear(1816,64)\n",
    "# # model.policy.mlp_extractor.value_net[0] = torch.nn.Linear(1816,64)\n",
    "\n",
    "# model.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22621-SP0 10.0.22621\n",
      "- Python: 3.7.0\n",
      "- Stable-Baselines3: 2.0.0\n",
      "- PyTorch: 1.13.1+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.21.6\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.28.1\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22621-SP0 10.0.22621\n",
      "- Python: 3.7.0\n",
      "- Stable-Baselines3: 2.0.0\n",
      "- PyTorch: 1.13.1+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.21.6\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.28.1\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiInputPolicy:\n\tsize mismatch for actor.latent_pi.0.weight: copying a param with shape torch.Size([256, 116]) from checkpoint, the shape in current model is torch.Size([256, 616]).\n\tsize mismatch for critic.qf0.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).\n\tsize mismatch for critic.qf1.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).\n\tsize mismatch for critic_target.qf0.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).\n\tsize mismatch for critic_target.qf1.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1688\\3025928868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SAC_fusion_model_run_2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_system_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\GE67HX   12UHS\\anaconda3\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    759\u001b[0m                 )\n\u001b[0;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m         \u001b[1;31m# put other pytorch variables back in place\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpytorch_variables\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GE67HX   12UHS\\anaconda3\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m             \u001b[1;31m# put state_dicts back in place\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexact_match\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[1;31m# Patch to load Policy saved using SB3 < 1.7.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GE67HX   12UHS\\anaconda3\\envs\\py37\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\u001b[0m in \u001b[0;36mset_parameters\u001b[1;34m(self, load_path_or_dict, exact_match, device)\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m                 \u001b[1;31m# Assume attr is th.nn.Module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m                 \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexact_match\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m             \u001b[0mupdated_objects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\GE67HX   12UHS\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1672\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiInputPolicy:\n\tsize mismatch for actor.latent_pi.0.weight: copying a param with shape torch.Size([256, 116]) from checkpoint, the shape in current model is torch.Size([256, 616]).\n\tsize mismatch for critic.qf0.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).\n\tsize mismatch for critic.qf1.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).\n\tsize mismatch for critic_target.qf0.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618]).\n\tsize mismatch for critic_target.qf1.0.weight: copying a param with shape torch.Size([256, 118]) from checkpoint, the shape in current model is torch.Size([256, 618])."
     ]
    }
   ],
   "source": [
    "# model = SAC.load(\"SAC_fusion_model_run_2\", print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SAC.load(\"SAC_model_run1_97\", print_system_info=True)\n",
    "\n",
    "for i in range(3,1000):\n",
    "    name = \"SAC_fusion_model_run_\"+str(i+1)\n",
    "    print(name)\n",
    "    print(model)\n",
    "    model.learn(total_timesteps=1000, log_interval=4)\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
